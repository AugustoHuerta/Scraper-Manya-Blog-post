# -*- coding: utf-8 -*-
"""Manya scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n0AxIWl9YuKX8Gb4fNZl6fRWbkCUgjdn

# Este es un scraper que recopila características esenciales de cada artículo del blog de Manya.pe

Este es un scraper que recopila:
1. El título.
2. El número de imágenes.
3. El nombre del autor.
4. El título del autor.
5. La fecha.
6. La url.

<p>De cada artículo del blog de Manya.pe en un archivo .csv.</p>

Es de libre uso y según el robots.txt de Manya.pe, está permitido.

¡Empezemos!

## Creación de nuestra sopa madre

Descargamos los módulos necesarios. (Google Colab ya tiene estos módulos por defecto)
"""

!pip install bs4
!pip install lxml
!pip install requests

"""Importamos las módulos necesarios para este proyecto"""

from bs4 import BeautifulSoup
import requests
import csv

"""Definimos la url en donde estará el listado de los artículos y le hacemos la request."""

url_main = 'https://manya.pe/blog/'
url_main_r = requests.get(url_main)

"""Revisamos que no exista problema con la request."""

if url_main_r.status_code == 200:
   print('No problem')
else:
  print('Uh. There is something wrong here')

"""Y creamos nuestra sopa:"""

soup = BeautifulSoup(url_main_r.text, 'lxml')

"""## Exploramos cómo obtener las urls de todos los blogpost

En este punto debemos encontrar cómo Manya organiza sus artículos,
buscar el tag en común que comparten y explotarlo para obtener las urls.

### El mapa al tesoro: El tag que agrupa todos los artículos de Manya

<p>En este caso encontramos que Manya agrupa todos sus blog post en tags "articles".</p>
<p>Aquí como:</p>

[![Imagen con dirreción a video explicativo](https://img.youtube.com/vi/uXYuaVxmSxQ/0.jpg)](https://youtu.be/uXYuaVxmSxQ)


---

Por lo que probamos encontrar todos los elementos con el tag 'article'.
"""

soup.find_all('article')

"""BINGO. Los elementos corresponden con los artículos vistos.
<p>Ahora los guardamos en la variable "articles".</p>
"""

articles = soup.find_all('article')

"""Y, por diversión, revisemos cuántos artículos ha publicado Manya en total:"""

len(articles)

"""### Enlistar todas las url de los blogs

Buscamos ahora en qué parte del tag "article" está el tag "a" que contiene los "href" (Las urls de cada artículo) para guardarlas en una lista.

---

Análisamos el output de el primer artículo de nuestra lista
"""

articles[0]

"""Encontramos que el "href" está ubicado en uno de los "a" tags. Por lo tanto buscamos todos los "a" tags"""

articles[94].find_all('a')

"""Y entendemos que el "href" está en el segundo y tercer tag. Por lo tanto lo obtenemos con la siguiente línea:"""

articles[94].find_all('a')[2].get('href')

"""Con este dato, enlistamos todas las urls de todos los artículo con un list comprenhension:"""

url_articles = [articles[i].find_all('a')[2].get('href') for i in range(len(articles))]

"""Corroboramos nuestra lista:"""

url_articles

"""Bingo!

## Obtener las características principales de cada artículo

Ahora debemos crear un función que nos permita obtener:
1. El título.
2. El número de imágenes.
3. El nombre del autor.
4. El título del autor.
5. La fecha del artículo.
6. La url del artículo.

<p>De cada artículo.</p>
<p>¡Comenzemos!</p>

### Creamos la sopa del artículo.

<p>En este caso usamos el artículo número 10 ("Barriga llena, cliente contento: conoce el caso de éxito Panza Brava") para hacer nuestro análisis.</p>

<p>¿Por qué este artículo? Es el primero que encontre con imágenes dentro del artículo.</p>
"""

url = url_articles[9]
try:
  url_article_request = requests.get(url)
  sopita = BeautifulSoup(url_article_request.text,'lxml')
  print("Sopa lograda")
except Exception as error:
  print("Ocurrío un error al hacer la request")
  print(error)

"""### Obtenemos las palabras y número de imágenes del artículo

En este caso encontramos que el texto se encuentra en el primer sub-tag "div" del main-tag "div" con un data-id de "d6c4145".

(Como nota personal, me dí cuenta que usar la func "find" para el tag "div class="elementor-widget-wrap"" me daba 45 tags con estas mismas características para el blog de Manya. Entonces para encontrar el que yo quería me fui a un "div" que tuviera un "id" único para el html pero igual para TODOS los artículos)
"""

main_article = sopita.find('div', {'data-id':'d6c4145'}).find('div')

len(sopita.find_all('div', {'class':'elementor-widget-container'}))

"""Ahora para encontrar el número de imágenes solo debemos buscar todos los tags "img" en el texto y pasarle un len()."""

len(main_article.find_all('img'))

"""Luego guardamos todas las palabras del texto en una variable "words" en 2 pasos:
1. Le asignamos cada tag de todo el texto a la variable "words" con un list comprenhension.
2. Guardamos cada texto de cada tag en nuestra variable "words" con otros list comprenhension
"""

words = [tag for tag in main_article]
words = [tag.get_text() for tag in words]

"""Luego juntamos todas las palabras en la variable "all_strs", todas en un único string con el método "join"."""

all_strs = "".join(words)

"""Corroboramos que tengamos todo el texto del artículo."""

all_strs

"""Luego obtenemos el número de las palabrac con "len()" y el método "split"."""

word_count = len(all_strs.split())
word_count

"""### Obtenemos las demás características:

Encontramos en dónde están ubicados nuestros datos para cada característica.
- Por ejemplo el caso del nombre del autor se encuentra en el tag "h4" del tag "div class='elementor-author-box__text'"
"""

autor = sopita.find_all('div', class_='elementor-author-box__text')[0]
autor_name = autor.find('h4').get_text()
autor_job = autor.find('div',class_="elementor-author-box__bio").get_text().strip()
print(autor_name,",", autor_job)

"""Ahora lo aplicamos para cada característica que queremos ver:"""

date = sopita.find_all('span', class_='elementor-icon-list-text elementor-post-info__item elementor-post-info__item--type-date')[0].get_text().strip()
date

tittle = sopita.find_all('h1')[0].get_text().strip()
tittle

"""## Guardar las características de cada artículo en un .csv

### Crear la función que encapsule todo el proceso de arriba:

Esta nos retorna un dic con cada key como característica y cada value como valor de la key.
"""

def article_chars(url):
    url_article_request = requests.get(url)
    sopita = BeautifulSoup(url_article_request.text,'lxml')
    text = sopita.find('div', {'data-id':'d6c4145'}).find('div')
    words = [tag for tag in text]
    words = "".join([tag.get_text() for tag in words])
    word_count = len(words.split())
    autor = sopita.find_all('div', class_='elementor-author-box__text')[0]
    autor_name = autor.find('h4').get_text().strip()
    autor_job = autor.find('div',class_="elementor-author-box__bio").get_text().strip()
    date = sopita.find_all('span', class_='elementor-icon-list-text elementor-post-info__item elementor-post-info__item--type-date')[0].get_text().strip()
    tittle = sopita.find_all('h1')[0].get_text().strip()
    image_count = len(text.find_all('img')) + 1
    return {'Tittle':tittle,
            'Word_count': word_count,
           'Image_count':image_count,
            'Autor': autor_name,
            'Autor_job': autor_job,
            'Date': date,
            'Url' : url
           }
article1 = article_chars('https://manya.pe/todo-lo-que-tienes-que-saber-para-un-adecuado-mantenimiento-web/')
print(article1)

"""### Guardar todos estos dict en una lista"""

chars_all_articles = []
count = 0
for url in url_articles:
    count += 1
    article1 = article_chars(url)
    chars_all_articles.append(article1)
    print(f"Obtenido articulo N*{count}")

"""### Escribirlo en un archivo .csv

<p>Debido a que no existen como tal "columnas" en un archivo .csv (A diferencia de un excel) reconfiguré la lista para que puediera funcionar.<p>
<p>Lo hice con de esta manera:<p>
1. Crear una lista con los títulos de cada característica (Tittle, name_autor, etc)<p>
2. Luego obtener una lista conteniendo cada valor de cada dict que obtuvimos en el paso anterior.<p>
3. Juntarlas y usar esta lista hija para añadir rows en nuestro archivo.
"""

chars_all_articles1 = [list(chars_all_articles[0].keys())] + [list(dicter.values()) for dicter in chars_all_articles]

"""Luego ejecutamos esta línea de código para poder guardar el archivo "test.csv" en tu drive.
<p>Pedira tu autorización y solo le das aceptar
"""

from google.colab import drive
drive.mount('/content/drive/')

"""Y creamos nuesro archivo "test.csv", obteniendo en cada línea las características de cada artículo."""

with open('/content/drive/MyDrive/test.csv', 'w') as f:
    writer = csv.writer(f)
    for i in range(len(chars_all_articles1)):
        print(chars_all_articles1[i])
        tittle = chars_all_articles1[i][0]
        Word_count = chars_all_articles1[i][1]
        Image_count = chars_all_articles1[i][2]
        Autor = chars_all_articles1[i][3]
        Autor_job = chars_all_articles1[i][4]
        Date =chars_all_articles1[i][5]
        Url = chars_all_articles1[i][6]
        writer.writerow([tittle,Word_count,Image_count,Autor,Autor_job,Date,Url])